---
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(knitr)
```

# Pre-Processing & Model Selection

## Data Pre-Processing

Based on the EDA (Exploratory Data Analysis) performed earlier and variable descriptions, it can be inferred that there are no missing values. However, the variables were of different data types. In order to perform operations on data, we need to ensure consistency of data types. *Define transformation*. The following table shows different variables in the dataset and the respective transformation performed on each of them.

| Data Type | Variable                                     | Transformation performed | Technique used       |
|-----------|-----------------------------------------------|----------------|------------------|
| Numerical | Wife's age, Number of children ever born      | Scaling        | Standard Scaling |
| Ordinal   | Wife's education, Husband Education,          | Encoding       | Ordinal Encoding |
|           | Husband's Occupation,Standard of living Index |                |                  |
| Binary    | Wife's religion, Wife working Media Exposure  | None           | Pass through     |

## Finding the best Model:
Our intent here is to find the probability of ___ and 
The target variable (Contraceptive method used) has three values:
```
1 = No-use
2 = Long-term, and
3 = Short-term
```

For simplicity and better model performance. We have combined `2 = Long-term`, `3 = Short-term` into one class and it was given a value of `1`. And the label `1 = No-use` was given a value of `0`, beacuse our aim is to predict the use(long or short)/ no use of contraceptives .

Our target distribution now have 
* `0 = No-use` : **445** observations, 
* `1 = use` : **586** observations.

With this data, our problem statement now turns into binary classification problem. The algorithms we used and tried in the process of finding the best model are:

1.  Decision Tree
2.  kNN
3.  Logistic Regression
4.  RBF SVC

## Results of Cross Validation

From the table \@ref(tab:crossVal), it can be clearly inferred that the RBF SVC algorithm is giving us the best score on both training and cross val dataset. The metric used to evaluate the cross validation was **accuracy**.

```{r crossVal, echo=FALSE }
cross_val <- read.csv("../results/val_score_results.csv") 
kable(cross_val, caption="Cross Validation Result (Score for Accuracy)")
```

## Hyper-Parameter Optimization

Given the performance of RBF SVC was the best, it was chosen for hyper-parameter tuning. The results of the top 5 models are shown in \@ref(tab:hyperparam). 

It can be observed that the best parameters are: `C = 10.0`, `gamma = 0.01`.

```{r hyperparam, echo=FALSE}
hyperparameter <- read.csv("../results/Random_Search_results.csv") 
kable(hyperparameter, caption="Hyperparameter Selection")
```
