---
output:
  html_document: default
  pdf_document: default
---
# Model Testing
After finding out the best parameters by optimizing the accuracy score, we tried the model on the test data set.In Total there were 442 Observations in the test split. 

<<<<<<< HEAD
a) 0=No Use = 184
b) 1=Use = 258

## Confusion Matrix:

We studies the confusion matrix to understand the model's predictive power\@ref(fig:confusionmat):
=======
The best parameters were identified by optimizing the accuracy score and subsequently, the model was tried on the test data set. There were **442** Observations in the test split in total.

* `0 = No Use` - **184** observations
* `1 = Use` - **258** observations

## Confusion Matrix:

A study of the confusion matrix will provide an understanding into the model's predictive power from the figure \@ref(fig:confusionmat):
>>>>>>> 415ea14d0397fd3bfa49bd45949c09eaeeb3a5c5

```{r confusionmat, echo=FALSE, fig.cap="Confusion Matrix (Actual vs Predicted)", out.width = '50%'}

knitr::include_graphics("../results/cm.png")

```

<<<<<<< HEAD
Look at the confusion matrix we can see that model is predicting well, the accuracy is 0.74. However, there are also some false -ve and false +ve error that is coming, so there are still some scope of improvement.

## Scoring Metric:

We also looked at the recall, precision and the f1-score \@ref(fig:scoringmet)::
=======
By looking at the confusion matrix, it can be observed that the model is predicting well on the total number of `True positives` i.e 231 and `True Negatives` i.e 97 . However, there are some false +ve and -ve observed as well.  

`False positives` are indicated when we affirmatively predict the usage of contraceptives when in fact, the person does not use contraceptives i.e in our matrix 87 .

## Scoring Metric:

The recall, precision and the f1-score were observed while considering each class to be the positive class. The recall value of **0.90** indicates a good true positive rate (TPR) for the `1` class while the **0.53** indicates the TPR of the `0` class. These cumulative scores can be found in `macro avg` and `weighted avg` in the table \@ref(tab:scoringmet).
>>>>>>> 415ea14d0397fd3bfa49bd45949c09eaeeb3a5c5

```{r scoringmet, echo=FALSE, fig.cap="Scoring Metrics", out.width = '50%'}
knitr::include_graphics("../results/cl_report.png")
```


## Precision-Recall Curve:

<<<<<<< HEAD
Figure \@ref(fig:precrec)
=======
The precision and recall tradeoff of our model could be observed by plotting the PR curve with the mean Average Precision score. A good enough AP score of 0.79 could be observed from the figure \@ref(fig:precrec).
>>>>>>> 415ea14d0397fd3bfa49bd45949c09eaeeb3a5c5

```{r precrec, echo=FALSE, fig.cap="Precision vs Recall Curve", out.width = '50%'}
knitr::include_graphics("../results/pr_curve.png")
```


<<<<<<< HEAD
## ROC Curve
=======
In order to obtain an overall score for our model, the Area under the curve was observed which resulted in a decent score of 78% from the figure \@ref(fig:roccurve).
>>>>>>> 415ea14d0397fd3bfa49bd45949c09eaeeb3a5c5

Figure \@ref(fig:roccurve)
```{r roccurve, echo=FALSE, fig.cap="AUC ROC Curve", out.width = '100%'}
knitr::include_graphics("../results/roc_curve.png")
```

## Final Conclusion


<<<<<<< HEAD
We have tried 4 different models, the intent was to predict the use of contraceptive based on socio-economic and eduction level. As we can see from the above parameters, The model is performing well with an accuracy of $0.74$ and recall of $0.9$, precision of $0.73$ and f1_score of $0.8$ and AUC $78%$. This is in line with our training model.The very high recall shows that there are less number of false negatives.

Nevertheless, there are still cases where the model is not predicting correctly, and hence there is scope of improvement, before we deploy it in a real world scenario.

=======
The intent of the process was to predict the use of contraceptives in women based on socio-economic and education levels. In the process, 4 different models were tried. It can be observed from the above parameters that the chosen model is indeed performing well with an accuracy of $74\%$ , `recall` of $90\%$, `precision` of $73\%$ , `f1_score` of $80\%$ and `AUC` $78\%$. These results are in line with the validation scores outlined previously. The high `recall value` of $90\%$ indicates that `False Negatives` are very low. 

However, it could be also be observed that there are indeed cases where the model is not predicting correctly, and hence there is scope for improvement before the model is deployed in a real world scenario.
>>>>>>> 415ea14d0397fd3bfa49bd45949c09eaeeb3a5c5
