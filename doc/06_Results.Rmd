---
output:
  html_document: default
  pdf_document: default
---
# Model Testing
After finding out the best parameters by optimizing the accuracy score, we tried the model on the test data set.In Total there were 442 Observations in the test split. 

a) 0=No Use = 184
b) 1=Use = 258

## Confusion Matrix:

A study of the confusion matrix will provide an understanding into the model's predictive power from the figure \@ref(fig:confusionmat):

```{r confusionmat, echo=FALSE, fig.cap="Confusion Matrix (Actual vs Predicted)", out.width = '50%'}

knitr::include_graphics("../results/cm.png")

```

## Scoring Metric:

The recall, precision and the f1-score were observed while considering each class to be the positive class. The recall value of **0.90** indicates a good true positive rate (TPR) for the `1` class while the **0.53** indicates the TPR of the `0` class. These cumulative scores can be found in `macro avg` and `weighted avg` in the table \@ref(tab:scoringmet).

```{r scoringmet, echo=FALSE, fig.cap="Scoring Metrics", out.width = '50%'}
cl_report <- read.csv(file = '../results/cl_report.csv')
knitr::kable(cl_report, caption = "Scoring Metrics")
```


## Precision-Recall Curve:

The precision and recall trade-off of our model could be observed by plotting the PR curve with the mean Average Precision score. A good enough AP score of 0.79 could be observed from the figure \@ref(fig:precrec).

```{r precrec, echo=FALSE, fig.cap="Precision vs Recall Curve", out.width = '50%'}
knitr::include_graphics("../results/pr_curve.png")
```

## ROC Curve

In order to obtain an overall score for our model, the Area under the curve was observed which resulted in a decent score of 78% from the figure \@ref(fig:roccurve).

```{r roccurve, echo=FALSE, fig.cap="AUC ROC Curve", out.width = '100%'}
knitr::include_graphics("../results/roc_curve.png")
```

## Final Conclusion
The intent of the process was to predict the use of contraceptives in women based on socio-economic and education levels. In the process, 4 different models were tried. It can be observed from the above parameters that the chosen model is indeed performing well with an accuracy of $74\%$ , `recall` of $90\%$, `precision` of $73\%$ , `f1_score` of $80\%$ and `AUC` $78\%$. These results are in line with the validation scores outlined previously. The high `recall value` of $90\%$ indicates that `False Negatives` are very low. 

However, it could be also be observed that there are indeed cases where the model is not predicting correctly, and hence there is scope for improvement before the model is deployed in a real world scenario.

