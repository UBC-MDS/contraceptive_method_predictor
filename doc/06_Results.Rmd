---
output:
  html_document: default
  pdf_document: default
---

# Model Testing

The best parameters were identified by optimizing the accuracy score and subsequently, the model was tried on the test data set. There were **442** Observations in the test split in total.

* `0 = No Use` - **184** observations
* `1 = Use` - **258** observations

## Confusion Matrix:

A study of the confusion matrix will provide an understanding into the model's predictive power from the figure \@ref(fig:confusionmat):

```{r confusionmat, echo=FALSE, fig.cap="Confusion Matrix (Actual vs Predicted)", out.width = '50%'}

knitr::include_graphics("../results/cm.png")

```

By looking at the confusion matrix, it can be observed that the model is predicting well on the total number of `True positives` i.e 231 and `True Negatives` i.e 97 . However, there are some false +ve and -ve observed as well.  

`False positives` are indicated when we affirmatively predict the usage of contraceptives when in fact, the person does not use contraceptives i.e in our matrix 87 .

## Scoring Metric:

The recall, precision and the f1-score were observed while considering each class to be the positive class. The recall value of **0.90** indicates a good true positive rate (TPR) for the `1` class while the **0.53** indicates the TPR of the `0` class. These cumulative scores can be found in `macro avg` and `weighted avg` in the table \@ref(tab:scoringmet).

```{r scoringmet, echo=FALSE, tab.cap="Scoring Metrics", out.width = '50%'}
# knitr::include_graphics("../results/cl_report.png")
cl_report <- read.csv(file = '../results/cl_report.csv')
knitr::kable(cl_report, caption = "Scoring Metrics")
```

## Precision-Recall Curve:

The precision and recall tradeoff of our model could be observed by plotting the PR curve with the mean Average Precision score. A good enough AP score of 0.79 could be observed from the figure \@ref(fig:precrec).

```{r precrec, echo=FALSE, fig.cap="Precision vs Recall Curve", out.width = '50%'}
knitr::include_graphics("../results/pr_curve.png")
```

## ROC Curve

In order to obtain an overall score for our model, the Area under the curve was observed which resulted in a decent score of 78% from the figure \@ref(fig:roccurve).

```{r roccurve, echo=FALSE, fig.cap="AUC ROC Curve", out.width = '100%'}
knitr::include_graphics("../results/roc_curve.png")
```

# Final Conclusion

The intent of the process was to predict the use of contraceptives in women based on socio-economic and education levels. In the process, 4 different models were tried. It can be observed from the above parameters that the chosen model is indeed performing well with an accuracy of $74\%$ , `recall` of $90\%$, `precision` of $73\%$ , `f1_score` of $80\%$ and `AUC` $78\%$. These results are in line with the validation scores outlined previously. The high `recall value` of $90\%$ indicates that `False Negatives` are very low. 

However, it could be also be observed that there are indeed cases where the model is not predicting correctly, and hence there is scope for improvement before the model is deployed in a real world scenario.
