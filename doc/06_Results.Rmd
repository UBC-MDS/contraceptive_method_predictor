---
output:
  html_document: default
  pdf_document: default
---
# Model Testing
After finding out the best parameters by optimizing the accuracy score, we tried the model on the test data set. 

## Confusion Matrix:

A study of the confusion matrix will provide an understanding into the model's predictive power from the figure \@ref(fig:confusionmat):

```{r confusionmat, echo=FALSE, fig.cap="Confusion Matrix (Actual vs Predicted)", out.width = '50%'}

knitr::include_graphics("../results/cm.png")

```

We have considered the use of contraceptive method as positive class.By looking at the confusion matrix, it can be observed that the model is predicting well on the total number of `True positives` i.e 231  which are the ones that the model predicted correctly to be using contraceptive method.and `True Negatives` i.e 97 which denotes correctly for not using contraceptive method. However, there are some false +ve and -ve observed as well.  

`False positives` are indicated when we affirmatively predict the usage of contraceptives when in fact, the person does not use contraceptives i.e in our matrix 87 .and `False Negatives` indicated when we incorrectly predict the person is not using, when they are actually using contraceptives.

## Scoring Metric:

The recall, precision and the f1-score were observed while considering each class to be the positive class. The recall value of **0.90** indicates a good true positive rate (TPR) for the `1` class while the **0.53** indicates the TPR of the `0` class. These cumulative scores can be found in `macro avg` and `weighted avg` in the table \@ref(tab:scoringmet).

```{r scoringmet, echo=FALSE, fig.cap="Scoring Metrics", out.width = '50%'}
cl_report <- read.csv(file = '../results/cl_report.csv')
knitr::kable(cl_report, caption = "Scoring Metrics")
```


## Precision-Recall Curve:


The precision and recall trade-off of our model could be observed by plotting the PR curve with the mean Average Precision score. A good enough AP score of 0.79 could be observed from the figure \@ref(fig:precrec).


```{r precrec, echo=FALSE, fig.cap="Precision vs Recall Curve", out.width = '50%'}
knitr::include_graphics("../results/pr_curve.png")
```

## ROC Curve

As mentioned above, the curves were plotted at a threshold of $0.5$. In order to obtain an overall score for our model, the Area under the curve was observed which resulted in a decent score of 78% from the figure \@ref(fig:roccurve).

```{r roccurve, echo=FALSE, fig.cap="AUC ROC Curve", out.width = '100%'}
knitr::include_graphics("../results/roc_curve.png")
```
## Feature Importance

We have predicted the co-efficients/ weights of each features and found that the features Number_of_children_ever_born, wife_age, wife_education are considered to be top three features of importance.

```{r feature_imp, echo=FALSE, fig.cap="Feature Importances", out.width='100%'}
knitr::include_graphics("../results/feature_imp.png")
```
The other features like standard_of_living_index, Husband_occupation, wife_religion, Husband_education also seem to pay a significant role. Media_exposure, surprisingly seem to not play any importance, but it may be due to bias in the methods involved in data collection.Also as we can saw in the EDA part, features like standard of living, Husband's education  were coming as prominent ones. 

# Final Conclusion

The intent of the process was to predict the use of contraceptives in married woman based on socio-economic and education levels. In the process, 4 different models were tried. It can be observed from the above parameters that the chosen model is indeed performing well(given the size of the data-set) with an accuracy of $74\%$ , `recall` of $90\%$, `precision` of $73\%$ , `f1_score` of $80\%$ and `AUC` $78\%$. The precision indicates, that out of all predicted positives,how many are actually positive, ie. out of 100 predicted positive samples, 73 were actually using the contraceptives. Recall indicates, out of total positives how many are predicted positive A high recall shows that our model could identify most of them correctly.
These results are in line with the validation scores outlined previously. The high `recall value` of $90\%$  also indicates that `False Negatives` are very low and an appreciable f1_score of 0.8.


However, it could be also be observed that there are indeed cases where the model is not predicting correctly, and hence there is scope for improvement before the model is deployed in a real world scenario.

